---
title: Manifold Clustering in the Generalized Random Dot Product Graph

# to produce blinded version set to 1
blinded: 0

authors: 
- name: John Koo
  affiliation: Department of YYY, University of XXX

keywords:
- block models, community detection, coordinate descent, latent structure models, manifold clustering, random dot product graph

abstract: |
  The text of your abstract. 200 or fewer words.

bibliography: bibliography.bib
output: rticles::asa_article
urlcolor: blue
header-includes:
- \usepackage{float}
- \usepackage{mathtools}
- \usepackage{natbib}
- \usepackage[linesnumbered,ruled,vlined]{algorithm2e} 
- \usepackage{verbatim}
- \usepackage{amsthm}
- \usepackage{comment}
- \usepackage{amsfonts}
---

\newcommand{\diag}{\mathrm{diag}}
\newcommand{\tr}{\mathrm{Tr}}
\newcommand{\blockdiag}{\mathrm{blockdiag}}
\newcommand{\indep}{\stackrel{\mathrm{ind}}{\sim}}
\newcommand{\iid}{\stackrel{\mathrm{iid}}{\sim}}
\newcommand{\Bernoulli}{\mathrm{Bernoulli}}
\newcommand{\Betadist}{\mathrm{Beta}}
\newcommand{\BG}{\mathrm{BernoulliGraph}}
\newcommand{\Uniform}{\mathrm{Uniform}}
\newcommand{\PABM}{\mathrm{PABM}}
\newcommand{\RDPG}{\mathrm{RDPG}}
\newcommand{\GRDPG}{\mathrm{GRDPG}}
\newcommand{\Multinomial}{\mathrm{Multinomial}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\as}{\stackrel{\mathrm{a.s.}}{\to}}
\newcommand{\ER}{\text{Erd\"{o}s-R\'{e}nyi}}

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      # eval = FALSE,
                      comment = NA, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.pos = 'H', 
                      fig.align = 'center', 
                      fig.height = 3, 
                      fig.width = 5, 
                      fig.dpi = 300)

options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')
```

```{r}
import::from(magrittr, `%>%`, `%<>%`)
library(ggplot2)
library(gganimate)
source('../functions.R')
source('~/dev/pabm-grdpg/functions.R')
source('https://mtrosset.pages.iu.edu/Courses/675/graph.r')

theme_set(theme_bw())

doMC::registerDoMC(parallel::detectCores())
```

# Introduction

We define a *Bernoulli graph* as a random graph model for which edge probabilities are contained in an edge probability matrix $P \in [0, 1]^{n \times n}$, and an edge occurs between vertices $i$ and $j$ with probability $P_{ij}$. 
Common random graph models then impose structure on $P$, based on various assumptions about the way in which the data are generated, or to allow $P$ to be estimated. 
One example is the $\ER$ model, in which all edge probabilities are fixed, i.e., $P_{ij} = p$. 

One common analysis task for graph and network data is community detection, which assumes that each vertex of a graph has a hidden community label. The goal of the analysis is then to estimate these labels upon observing a graph. 
In order to perform this analysis as a statistical inference task is to define a probability model with inherent community structure. We call such models *block models*: 
First, each vertex is assigned a label $z_1, ..., z_n \in \{1, 2, ..., K\}$ where $K \ll n$. 
Then each edge probability $P_{ij}$ is said to depend on the labels $z_i$ and $z_j$, possibly along with some other parameters. 
For example, the stochastic block model (SBM) sets a fixed edge probability for each pair of communities, i.e., $P_{ij} = \omega_{z_i, z_j}$. 
The degree-corrected block model (DCBM) assigns an additional parameter $\theta_i$ to each vertex by which edge probabilities are scaled, i.e., $P_{ij} = \theta_i \theta_j \omega_{z_i, z_j}$. 
The popularity adjusted block model (PABM) assigns $K$ parameters to each vertex $\lambda_{i1}, \lambda_{i2}, ..., \lambda_{iK}$ that describe that vertex's affinity toward each community; 
the edge probability between vertices $i$ and $j$ is then defined as the product of vertex $i$'s affinity toward vertex $j$'s community and vertex $j$'s affinity toward vertex $i$'s community, i.e., $P_{ij} = \lambda_{i z_j} \lambda_{j z_i}$. 

The three block model types, as well as the $\ER$ model, impose structure on $P$, including on the rank of $P$. 
$P$ has rank 1 for the $\ER$ model, rank $K$ for the SBM and DCBM, and rank $K^2$ for the PABM. 
This provides the intuition behind another family of Bernoulli graphs called the *random dot product graph* (RDPG) and *generalized random dot product graph* (GRDPG). 
In the RDPG, each vertex has a corresponding latent vector in $d$-dimensional Euclidean space, where $d$ is the rank of $P$ and $P$ is positive semidefinite. Then the edge probability between each pair of vertices is defined as the inner product between the corresponding latent vectors, i.e., $P_{ij} = x_i^\top x_j$. If the latent vectors are collected in a data matrix $X = \bigl[ x_1 \mid \cdots \mid x_n \bigr]^\top$, then the edge probability matrix for the RDPG is $P = X X^\top$. 
Similarly, the edge probability between each pair of vertices for the GRDPG is defined as the indefinite inner product between the corresponding latent vectors, i.e., $P_{ij} = x_i^\top I_{p,q} x_j$, where $I_{p,q} = \blockdiag(I_p, -I_q)$ and $p + q = d$. Then the edge probability matrix for the GRDPG is $P = X I_{p,q} X^\top$. This allows for a model similar to the RDPG for non-positive semidefinite $P$. 
While the RDPG and GRDPG do not necessarily have community structure, it has been shown that block models are specific cases of the RDPG or GRDPG in which latent vectors are organized by community. This includes the SBM, in which communities correspond to point masses, DCBM, in which communities correspond to line segments, and PABM, in which communities correspond to orthogonal subspaces. In this work, we extend this idea to communities organized into more general latent structures. In particular, we assume that each community corresponds to a manifold in the latent space.

# Generalized Random Dot Product Graphs with Community Structure

All Bernoulli graphs are generalized random dot product graphs. 
Whether this is useful for inference depends on the structure of the latent space. 
In the case of the $\ER$ model, SBM, DCBM, and PABM, the latent structure is linear, and the linearity can be exploited for community detection and parameter estimation. 
In this section, we discuss general, often nonlinear latent structure models, focusing on those with community structure. 

To motivate this, consider a generalization of the $\ER$ model. 
Recall that when viewed as an RDPG, the latent space of an $\ER$ model consists of one point in Euclidean space. 
In the following example, instead of fixing the edge probability, it is sampled from a distribution in such a way that when viewed as an RDPG, the latent space consists of a curve. 

\begin{example}[Hierarchical $\ER$ model]
In the $\ER$ model, the edge probability matrix has a fixed value $[P_{ij}] \equiv p \in [0, 1]$. 

Suppose that we have a random dot product graph in which the latent space is $\mathbb{R}^2$ and latent vectors are drawn uniformly from the quarter circle defined by $g(t) = \begin{bmatrix} \cos(\frac{\pi}{2} t) & \sin(\frac{\pi}{2} t) \end{bmatrix}^\top$, $0 \leq t \leq 1$. 
Then it can be shown that in this model, instead of a fixed $P_{ij} = p$, the edge probabilities are distributed with density $f(p) = \frac{2}{\pi - 2} \Big(\frac{1}{\sqrt{1 - p^2}} - 1 \Big)$. 
\end{example}

By changing the latent structure from a point mass to a curve, we are able to come up with more flexible Bernoulli graph models in which edge probabilities follow more general probability distributions. 
Community structure then can be added by sampling latent vectors from multiple curves. 
Then the adjacency spectral embedding of the resulting graph allows us to recover that community structure. 
This is illustrated in the following example. 

\begin{example}
Define two one-dimensional manifolds in $\mathbb{R}^2$ by $f_1(t) = \begin{bmatrix} \cos(\frac{\pi}{3} t) & \sin(\frac{\pi}{3} t) \end{bmatrix}^\top$ and $f_2(t) = \begin{bmatrix} 1 - \cos(\frac{\pi}{3} t) & 1 - \sin(\frac{\pi}{3} t) \end{bmatrix}^\top$.
Draw $t_1, ..., t_n \iid \Uniform(0, 1)$ and $z_1, ..., z_n \iid \Multinomial(\frac{1}{2}, \frac{1}{2})$, and compute latent vectors $x_i = f_{z_i}(t_i)$, which are collected in data matrix $X = \begin{bmatrix} x_1 & \cdots & x_n \end{bmatrix}^\top$. 
Finally, let $A \sim \RDPG(X)$. Fig. \ref{fig:example1} shows the latent configuration drawn from this latent distribution, a random dot product graph drawn from the latent configuration, and the adjacency spectral embedding of the graph. 
Although the community structure is not obvious from the graph, the embedding shows a clear separation between the two communities. 

```{r example1, fig.cap = 'Manifold block model described in Example 1. The latent configuration is on the left, a random dot product graph drawn from the latent configuration is on the middle, and the ASE is on the right.', fig.height = 2.5, fig.width = 7}
set.seed(123456)
n1 <- 2 ** 5
n2 <- 2 ** 5
n <- n1 + n2
z <- c(rep(1, n1), rep(2, n2))
u1 <- runif(n1)
u2 <- runif(n2)
x1 <- cos(pi / 3 * u1)
y1 <- sin(pi / 3 * u1)
x2 <- 1 + cos(pi / 3 * u2 + pi)
y2 <- 1 + sin(pi / 3 * u2 + pi)
data.matrix <- cbind(c(x1, x2), c(y1, y2))
A <- draw.graph(data.matrix %*% t(data.matrix))
Xhat <- embedding(A, 2, 0)

par(mfrow = c(1, 3))
# par(fig = c(0, .7, 0, 1))
plot(data.matrix, col = z * 2, asp = 1,
     xlab = NA, ylab = NA, 
     # xaxt = 'n', yaxt = 'n',
     xlim = c(-.1, 1.1), ylim = c(0, 1))
# ggplot() +
#   geom_point(aes(x = data.matrix[, 1], y = data.matrix[, 2], colour = factor(z))) +
#   labs(x = NULL, y = NULL) +
#   scale_colour_brewer(palette = 'Set1') +
#   theme(legend.position = 'none') +
#   coord_fixed()
# par(fig = c(.3, 1, 0, 1))
qgraph::qgraph(A, groups = factor(z), legend = FALSE)
plot(Xhat, col = z * 2, asp = 1, 
     # xaxt = 'n', yaxt = 'n', 
     xlab = NA, ylab = NA)
```
\end{example}

We now formally define the manifold block model. 

\begin{definition}[Manifold block model]
Let $p, q \geq 0$, $d = p + q \geq 1$, $1 \leq r < d$, $K \geq 2$, and $n \geq 1$ be integers.
Define manifolds $\mathcal{M}_1, ..., \mathcal{M}_K \subset \mathcal{X}$ for $\mathcal{X} = \{x, y \in \mathbb{R}^d : x^\top I_{p,q} y \in [0, 1] \}$ each by continuous function $g_k : [0, 1] \to \mathcal{X}$, 
and probability distributions $F_1, ..., F_K$ each with support $[0, 1]^r$. 
Then the following mixture model is a manifold block model: 

\begin{enumerate}
  \item Draw labels $z_1, ..., z_n \iid \Multinomial(\alpha_1, ..., \alpha_K)$. 
  \item Draw latent vectors by first drawing each $t_i \indep F_{z_i}$ and then compute each $x_i = g_{z_i}(t_i)$. 
  \item Let $X = \begin{bmatrix} x_1 & \cdots & x_n \end{bmatrix}^\top$, and draw $A \sim \RDPG(X)$ or $A \sim \GRDPG_{p,q}(X)$. 
\end{enumerate}
\end{definition}

# Methods

We provide two approaches to community detection for the manifold block model. 
First, we consider the case in which communities correspond to manifolds in the latent space that do not intersect and are separated by some finite distance. In this scenario, we use the convergence of the ASE to show that single linkage clustering on the latent space produces a clustering such that the total number of misclustered vertices goes to zero, with high probability. 

Next, we consider the case in which communities correspond to one-dimensional manifolds in the latent space and may or may not intersect. In this scenario, we propose an alternating coordinate descent algorithm that alternates between estimating the structure of the manifolds and the community labels, which we call $K$-curves clustering. We again use the convergence of the ASE to show that under certain conditions, $K$-curves clustering produces a clustering such that the proportion of misclustered vertices goes to zero, with high probability. 

## Nonintersecting Manifolds
\label{section:nonintersecting}

In this section, we consider the following scenario: 
Suppose that each community is represented by a closed manifold $\mathcal{M}_k$, $k \in \{1, ..., K\}$ in the latent space of a RDPG or GDRPG. 
Define $\delta = \min\limits_{k \neq \ell} \min\limits_{x \in \mathcal{M}_k, y \in \mathcal{M}_\ell} \|x - y\|$, the minimum distance between two manifolds. 
We assume that $\delta > 0$, i.e., the manifolds do not intersect. 

In the noiseless setting, if the subsample on each manifold is sufficiently dense, it is possible to construct for each manifold an $\eta_k$-neighborhood graph for each manifold for some $\eta_k > 0$ such that the graph is connected. 
Then if $\max_k \eta_k = \eta < \delta$, an $\eta$-neighborhood graph for the entire sample will consist of $K$ disconnected subgraphs that map onto each manifold. 
Equivalently, we can apply single-linkage clustering. 
The remainder of this section explores under which conditions these criteria are met for the latent configuration, in which latent vectors lie exactly on manifolds, as well as the ASE, which introduces noise. 

\begin{algorithm}[h]
\DontPrintSemicolon
\SetAlgoLined
\KwData{Adjacency matrix $A$, number of communities $K$, embedding dimensions $p$ and $q$.}
\KwResult{Community assignments $z_1, ..., z_n \in \{1, ..., K\}$.}
Compute $\hat{X}$, the ASE of $A$ using the $p$ most positive and $q$ most negative eigenvalues and their corresponding eigenvectors.\;
Apply single linkage clustering with $K$ communities on $\hat{X}$.\;
\caption{ASE clustering for nonintersecting communities.}
\end{algorithm}

Let $F_k$ be a probability distribution with support $\mathcal{M}_k$. 
Then we define a mixture model as follows:

1. Draw labels $z_1, ..., z_n \iid \Multinomial(\alpha_1, ..., \alpha_K)$. 
2. Draw latent vectors each as $x_i \indep F_{z_i}$ for distributions $F_1, ..., F_K$ with respective supports $\mathcal{M}_1, ..., \mathcal{M}_K$. 
3. Let $X = \begin{bmatrix} x_1 & \cdots & x_n \end{bmatrix}^\top$, 
and draw $A \sim \RDPG(X)$ or $A \sim \GRDPG_{p,q}(X)$. 

Note that here, we redefine the model to ignore $g_1, ..., g_K$, the parameterizations of each manifold. 
Instead, we sample points directly on the manifolds themselves. 
We will return to the parameterizations in Section \ref{section:intersecting}. 

\begin{theorem}[Community detection for nonintersecting manifolds without noise]
\label{nonintersect-no-noise}
Let $x_1, ..., x_n$ be points sampled on $K$ manifolds $\mathcal{M}_1, ..., \mathcal{M}_K$. 
Suppose $\delta = \min\limits_{k \neq \ell} \min\limits_{x_i \in \mathcal{M}_k, x_j \in \mathcal{M}_\ell} \| x_i - x_j \| > 0$. 
Define $A_n$ as the event that a $\eta$-neighborhood graph constructed from the sample $x_1, ..., x_n$ consists of exactly $K$ disconnected subgraphs that map exactly to each manifold for some $\eta \in (0, \delta)$. 
Then for any $\epsilon \in (0, 1)$, there exists an $N$ such that when $n > N$, $P(A_n) > 1 - \epsilon$. 
\end{theorem}

\begin{theorem}{Community detection for RDPG for which the communities come from nonintersecting manifolds}
\label{nonintersect-rdpg}
Let $x_1, ..., x_n$ be points sampled on $K$ manifolds $\mathcal{M}_1, ..., \mathcal{M}_K$. 
Suppose $\delta = \min\limits_{k \neq \ell} \min\limits_{x_i \in \mathcal{M}_k, x_j \in \mathcal{M}_\ell} \| x_i - x_j \| > 0$. 
Let $X = \begin{bmatrix} x_1 & \cdots & x_n \end{bmatrix}^\top$ and $A \sim \RDPG(X)$. 
Define $B_n$ as the event that an $\eta$-neighborhood graph constructed from the ASE of $A$ consists of exactly $K$ disconnected subgraphs that map exactly to each manifold for some $\eta \in (0, \delta)$. 
Then for any $\epsilon \in (0, 1)$, there exists an $N$ such that when $n > N$, $P(B_n) > 1 - \epsilon$. 
\end{theorem}

## Intersecting Manifolds
\label{section:intersecting}

In this section, we again consider the setting for the RDPG or GRDPG in which each community lies on a manifold in the latent space. 
However, this time, we do not assume that the manifolds are nonintersecting. 
We also restrict this case to one-dimensional manifolds which are each described by $g_k : [0, 1] \to \mathcal{X}$. 
Then we define a mixture model as follows:

1. Draw $t_1, ..., t_n \iid F$ for probability distribution $F$ with support $[0, 1]$.
2. Draw $z_1, ..., z_n \iid \Multinomial(\alpha_1, ..., \alpha_K)$, the community labels.
3. Let each $x_i = g_{z_i}(t_i)$ be the latent vector for vertex $v_i$, and collect the latent vectors into matrix $X = \begin{bmatrix} x_1 & \cdots & x_n \end{bmatrix}^\top$.
4. Draw $A \sim \RDPG(X)$ or $A \sim \GRDPG_{p,q}(X)$.

\begin{algorithm}[h]
\DontPrintSemicolon
\SetAlgoLined
\KwData{Adjacency matrix $A$, number of communities $K$, embedding dimensions $p$, $q$, stopping criterion $\epsilon$}
\KwResult{Community assignments $1, ..., K$, curves $g_1, ..., g_K$}
Compute $X$, the ASE of $A$ using the $p$ most positive and $q$ most negative eigenvalues and their corresponding eigenvectors.\;
Initialize community labels $z_1, ..., z_n$.\;
\Repeat {the change in $\sum_k \sum_{i \in C_k} \|x_i - g_k(t_i)\|^2$ is less than $\epsilon$} {
\For {$k = 1, ..., K$} {
Define $X_k$ as the rows of $X$ for which $z_i = k$.\;
Fit curve $g_k$ and positions $t_{k_i}$ to $X_k$ by minimizing $\sum_{k_i} \|x_{k_i} - g_k(t_{k_i})\|^2$.\;
}
\For {$k = 1, ..., K$} {
Assign $z_i \leftarrow \arg\min_\ell \|x_i - g_\ell(t_i)\|^2$.\
}
}
\caption{$K$-curves clustering.}
\end{algorithm}

\begin{theorem}
\label{k-curves-clustering}
Let each $g_k$ be smooth. 
Then $K$-curves clustering converges to a stationary point of the objective, 
$\sum_k \sum_{i \in C_k} \|x_i - g_k(t_i)\|^2$.
\end{theorem}

\begin{proof}
$K$-curves clustering is a batch coordinate descent algorithm. 
Thus, in order to show that it converges to a stationary point, it is sufficient to show that each descent step decreases the objective function. 
\end{proof}

$K$-curves clustering assumes that the functional form of $g_k$ is known. 
The choice of $g_k$ affects the difficulty of the algorithm. 
As a balance between flexibility and ease of estimation, we consider the case where each $g_k$ is a Bezier polynomial of degree $R$ with coefficients $p_k$. 
Then we have $g_k(t) = g(t; p_k) = \sum_{r=0}^R p_k^{(r)} \binom{R}{r} (1-t)^{R-r} t^r$. 

Given $\{t_i\}$ and $\{z_i\}$, it is straightforward to obtain $\hat{p}_k = \arg\min_p \sum_{k_i} \|x_{k_i} - g_k (t_{k_i}; p)\|^2$
$$\hat{p}_k = (T_k^\top T_k)^{-1} T_k^\top X_k,$$
where $T_k$ is an $n_k \times (R+1)$ matrix with rows $\begin{bmatrix} (1 - t_{k_i})^R & (1 - t_{k_i})^{R-1} t_{k_i} & \cdots & (1 - t_{k_i}) t_{k_i}^{R-1} & t_{k_i}^R \end{bmatrix}$.
Estimation of $\{t_i\}$ given $\{z_i\}$ and $\{p_k\}$ is more difficult. 
Each $t_i$ can be estimated separately: 
\begin{equation} \label{eq:min-t}
\hat{t}_i = \arg\min_t \|x_i - g(t; p_{z_i})\|^2. 
\end{equation}
This is equivalent to solving $0 = (x_i - g(t; p_{z_i}))^\top (\dot{g}(t; p_{z_i}))$. 
Setting $c^{(s)} = \sum_{r=0}^s (-1)^{s-r} \binom{R}{r} p^{(r)}_{z_i}$ for $s \neq 0$ and $c^{(0)} = p^{(0)}_{z_i} - x_i$, let $c = \begin{bmatrix} c^{(0)} & \cdots & c^{(R)} \end{bmatrix}^\top$. Then solving Eq. \ref{eq:min-t} is equivalent to finding the real roots of a polynomial with coefficients that are the sums of the reverse diagonals of $C D^\top$, where $C_{ij} = c_{ij} (-1)^i \binom{R}{i}$ and $D_{ij} = c_{i-1,j} (-1)^{i-1} \binom{R-1}{i-1}$. 

\begin{algorithm}[h]
\DontPrintSemicolon
\SetAlgoLined
\KwData{Adjacency matrix $A$, number of communities $K$, embedding dimensions $p$, $q$, stopping criterion $\epsilon$, $m_k \leq n_k$ known community assignments for each community}
\KwResult{Community assignments $1, ..., K$, curves $g_1, ..., g_K$}
Compute $X$, the ASE of $A$ using the $p$ most positive and $q$ most negative eigenvalues and their corresponding eigenvectors.\;
Fit curves $g_1, ..., g_K$ using each of the $m_1, ..., m_K$ points with known community labels by minimizing $\sum_{j=1}^{m_i} \|x_j - g_k(t_j)\|^2$.\;
Assign labels $z_1, ..., z_n$ to each $x_1, ..., x_n$ by minimizing $\|x_i - g_k(t_i)\|^2$ for $k$, holding the initial known labels constant.\; 
\Repeat {the change in $\sum_k \sum_{i \in C_k} \|x_i - g_k(t_i)\|^2$ is less than $\epsilon$} {
\For {$k = 1, ..., K$} {
Define $X_k$ as the rows of $X$ for which $z_i = k$.\;
Fit curve $g_k$ and positions $t_{k_i}$ to $X_k$ by minimizing $\sum_{k_i} \|x_{k_i} - g_k(t_{k_i})\|^2$.\;
}
\For {$k = 1, ..., K$} {
Assign $z_i \leftarrow \arg\min_\ell \|x_i - g_\ell(t_i)\|^2$, holding the known initial labels constant.\
}
}
\caption{Semi-supervised $K$-curves clustering.}
\end{algorithm}

\begin{theorem}
Let each $g(\cdot; p_k)$ be a nonintersecting Bezier polynomial of order $R$, 
and a GRDPG is drawn from vectors that lie on the curves. 
Suppose we observe the true labels of $m_k$ vertices from each community, and each $m_k > R + 1$. Suppose further that latent vectors $x_j = g(t_i; p_{z_j})$ that correspond to vertices with observed labels are such that 
Then as $n \to \infty$, the proportion of misclustered vertices from $K$-curves clustering approaches $0$ with probability $1$.
\end{theorem}

# Examples

\begin{example}
Here, $K = 2$ with $g_1(t) = \begin{bmatrix} t^2 & 2 t (1 - t) \end{bmatrix}^\top$ and $g_2(t) = \begin{bmatrix} 2 t (1 - t) & (1 - t) ^ 2 \end{bmatrix}^\top$. We draw $n_1 = n_2 = 2^8$ points uniformly from each curve. 

```{r, fig.height = 2, fig.cap = 'Latent positions, labeled by curve/community.'}
f1 <- function(t) {
  x1 <- t ^ 2
  x2 <- 2 * t - 2 * t ^ 2
  return(cbind(x1, x2))
}

f2 <- function(t) {
  x1 <- 2 * t - 2 * t ^ 2
  x2 <- 1 - 2 * t + t ^ 2
  return(cbind(x1, x2))
}

set.seed(314159)

n1 <- 2 ** 8
n2 <- n1
n <- n1 + n2
z <- c(rep(1, n1), rep(2, n2))

a <- 1
b <- 1
t1 <- rbeta(n1, a, b)
t2 <- rbeta(n2, a, b)
T <- rbind(t1, t2)

X1 <- f1(t1)
X2 <- f2(t2)
X <- rbind(X1, X2)

ggplot() + 
  geom_point(aes(x = X[, 1], y = X[, 2], colour = factor(z)), size = .5) + 
  coord_fixed() + 
  labs(x = expression(x[1]), y = expression(x[2]), colour = NULL) + 
  theme_bw() + 
  theme(legend.position = 'none')

P <- X %*% t(X)
diag(P) <- 0
A <- draw.graph(P)
Xhat <- embedding(A, 2, 0)
```

We draw $A \sim \RDPG(X)$ and obtain the following ASE:

```{r, fig.height = 3, fig.cap = 'ASE of an RDPG drawn from the latent positions, labeled by curve/community.'}
ggplot() + 
  geom_point(aes(x = Xhat[, 1], y = Xhat[, 2], colour = factor(z)), size = .5) + 
  coord_fixed() + 
  labs(x = expression(x[1]), y = expression(x[2]), colour = NULL) + 
  theme_bw() + 
  theme(legend.position = 'none')
```

We then try applying $K$-curves clustering to this graph. 
The first three are with random initial labels, forcing the intercept to be zero. 
The fourth initializes the labels randomly but allows the intercept to be nonzero. 
The fifth initializes the labels by spectral clustering with the normalized Laplacian, again forcing the intercept to be zero. 
The sixth also initializes via spectral clustering but allows the intercept to be nonzero. 

```{r}
animate <- FALSE
```

```{r, cache = TRUE}
maxit <- 50
animation.dir <- '~/dev/manifold-block-models/draft'
random.starts <- 2
eps <- 1e-6
normalize <- TRUE

out <- lapply(seq(random.starts), function(i) {
  intercept <- (i / random.starts > .5 + eps)
  set.seed(i)
  manifold.clustering(Xhat, 2, 
                      parallel = TRUE, 
                      intercept = intercept, 
                      maxit = maxit,
                      normalize = normalize, 
                      eps = 1e-3,
                      verbose = FALSE,
                      animate = animate,
                      animation.dir = animation.dir,
                      animation.title = paste('random', 
                                              i, 
                                              ifelse(intercept,
                                                     'w-intercept',
                                                     'no-intercept'),
                                              sep = '-'))
})
names(out) <- paste('random', seq(random.starts))
names(out) <- sapply(seq(random.starts), function(i) {
  if (i / random.starts < .5 + eps) {
    paste('random', i, '\n(w/o intercept)')
  } else {
    paste('random', i, '\n(w/ intercept)')
  }
})

out[['spectral clust\n(w/o intercept)']] <- manifold.clustering(
  Xhat, 2, 
  A = A, 
  parallel = TRUE, 
  intercept = FALSE, 
  normalize = normalize, 
  maxit = maxit, 
  eps = 1e-3,
  initialization = 'spectral',
  verbose = FALSE,
  animate = animate,
  animation.dir = animation.dir,
  animation.title = 'spec-clust-no-intercept')

out[['spectral clust\n(w/ intercept)']] <- manifold.clustering(
  Xhat, 2,
  A = A, 
  parallel = TRUE,
  intercept = TRUE,
  normalize = normalize, 
  maxit = maxit,
  eps = 1e-3,
  initialization = 'spectral',
  verbose = FALSE,
  animate = animate,
  animation.dir = animation.dir,
  animation.title = 'spec-clust-w-intercept')

out[['ground truth\n(w/o intercept)']] <- manifold.clustering(
  Xhat, 2,
  parallel = TRUE,
  intercept = FALSE,
  normalize = normalize, 
  maxit = maxit,
  eps = 1e-3,
  initialization = z,
  verbose = FALSE,
  animate = animate,
  animation.dir = animation.dir,
  animation.title = 'groundtruth-no-intercept')

out[['ground truth\n(w/ intercept)']] <- manifold.clustering(
  Xhat, 2,
  parallel = TRUE,
  intercept = TRUE,
  maxit = maxit,
  eps = 1e-3,
  initialization = z,
  verbose = FALSE,
  animate = animate,
  animation.dir = animation.dir,
  animation.title = 'groundtruth-w-intercept')
```

```{r, fig.cap = 'Clustering loss vs. iteration for each run of K-curve clustering.', fig.height = 2, fig.width = 6}
# max.iter <- max(sapply(out, function(x) x$niter))
max.loss <- max(sapply(out, function(x) max(x$loss[-1])))
# plot(out[[1]]$loss, xlim = c(0, max.iter), ylim = c(0, max.loss),
#      ylab = 'clustering loss', xlab = 'iteration')
# # lines(out[[1]]$loss)
# 
# for (i in seq_along(out)) {
#   points(out[[i]]$loss, col = i)
#   lines(out[[i]]$loss, col = i)
# }
# legend('topright', 
#        title = 'initialization',
#        cex = .75, 
#        bty = 'n',
#        legend = names(out),
#        col = seq_along(out),
#        pch = 1)


loss.df <- plyr::ldply(seq_along(out), function(i) {
  dplyr::tibble(loss = out[[i]]$loss,
                initialization = factor(names(out),
                                        levels = names(out))[i]) %>%
    dplyr::mutate(iteration = seq(dplyr::n()))
})

ggplot(loss.df) + 
  geom_point(aes(x = iteration, y = loss, colour = initialization)) + 
  geom_line(aes(x = iteration, y = loss, colour = initialization)) + 
  # ylim(0, max.loss) +
  # scale_y_log10() +
  labs(y = 'clustering loss')
```

```{r, fig.width = 6, fig.height = 5, fig.cap = 'ASE labeled by estimated community labels for each initialization strategy.'}
plots.list <- lapply(seq_along(out), function(i) {
  plot.estimated.curves(Xhat, out[[i]])
})

gridExtra::grid.arrange(grobs = plots.list, ncol = 3)
```

\end{example}

\begin{example}[Macaque visuotactile brain areas and connections \citep{https://doi.org/10.1111/j.1460-9568.2006.04678.x}]

```{r}
data(macaque, package = 'igraphdata')
A <- as.matrix(igraph::as_adjacency_matrix(macaque, type = 'both'))
A <- sign(A + t(A))
z <- as.numeric(factor(igraph::vertex_attr(macaque)$shape))

Xhat <- embedding(A, 2, 0)
n <- nrow(Xhat)
plot(Xhat, col = z, asp = 1)
```

```{r cache = TRUE}
set.seed(123)
z.spectral <- spectral.clustering(A)
macaque.out <- manifold.clustering(Xhat, 2, 
                                   # initialization = 'random',
                                   initialization = z.spectral,
                                   # initialization = z,
                                   intercept = FALSE, 
                                   parallel = TRUE, 
                                   verbose = FALSE, 
                                   animate = animate, 
                                   curve.init = 'x',
                                   animation.title = 'macaque')
```

```{r}
plot.estimated.curves(Xhat, macaque.out)
```

```{r}
plot(macaque.out$loss, type = 'l')
```

\end{example}

\begin{example}[Non-intersecting curves]

```{r}
n1 <- 2 ** 7
n2 <- 2 ** 7
n <- n1 + n2
z <- c(rep(1, n1), rep(2, n2))
u1 <- runif(n1)
u2 <- runif(n2)
x1 <- cos(pi / 3 * u1)
y1 <- sin(pi / 3 * u1)
x2 <- 1 + cos(pi / 3 * u2 + pi)
y2 <- 1 + sin(pi / 3 * u2 + pi)
data.matrix <- cbind(c(x1, x2), c(y1, y2))

plot(data.matrix, col = z * 2, asp = 1, xlab = NA, ylab = NA)
```

```{r}
P <- data.matrix %*% diag(c(1, 1)) %*% t(data.matrix)
A <- draw.graph(P)
Xhat <- embedding(A, 2, 0)
# plot(Xhat, col = z * 2, asp = 1)
```

```{r cache = TRUE}
z.init <- hclust(dist(Xhat), method = 'single') %>% 
  cutree(2)
nonintersect.out <- manifold.clustering(Xhat, 2, 
                                        initialization = z.init,
                                        # initialization = 'random', 
                                        intercept = TRUE, 
                                        animate = animate,
                                        animation.title = 'nonintersect')
```

```{r}
plot.estimated.curves(Xhat, nonintersect.out)
```

\end{example}

# Simulation Study

# Discussion

\appendix

\section{Proofs of Theorems}

In order to prove theorem \ref{nonintersect-no-noise}, we first establish that the density of points within one manifold is sufficiently dense. 
The following lemma is based on lemma 2 of \citet{trosset2020rehabilitating}.

\begin{lemma}
\label{lem:one-hypercube}
Let $x_1, ..., x_n \iid F$ with support $[0, 1]^r$, and $f(x) \geq a > 0$ everywhere on the support. 
Define $H_n$ as the event that an $\eta$-neighborhood graph constructed from the sample is connected for any $\eta > 0$. 
Then for any $\epsilon > 0$, there exists $N = O \bigg( \frac{\log \epsilon + r \log \eta - \frac{r}{2} \log r}{\log (1 - a \eta^r r^{-r / 2})} \bigg)$ such that $P(H_n) > 1 - \epsilon$ when $n \geq N$.
\end{lemma}

\begin{proof}
Divide the hypercube $[0, 1]^r$ into a grid of sub-hypercubes of side length at most $\eta / \sqrt{r}$. 
$E_n$ is satisfied if each sub-hypercube contains at least one $X_i$ from the sample. 

$$
\begin{aligned}
P(H_n) & = 1 - P(\text{some cells don't contain } X_i) \\
& \geq 1 - \sum_m^{\lceil \sqrt{r} / \eta \rceil^r} \prod_i^n P(X_i \text{ is not in the } k^{th} \text{ hypercube}) \\
& \geq 1 - \lceil \sqrt{r} / \eta \rceil^r (1 - a \eta^r / r^{r/2})^n,
\end{aligned}
$$
which approaches $1$ as $n \to \infty$. 
Setting this quantity as  $\geq 1 - \epsilon$ and solving for $n$ yields the desired rate. 
\end{proof}

Lemma \ref{lem:one-hypercube} extends to the case in which vectors are drawn from hypercubes with noise, given that the noise is bounded. 

\begin{lemma}
\label{lem:one-hypercube-noise}
Suppose the setup from lemma \ref{lem:one-hypercube}, but instead of observing each $x_i$, we observe each $y_i = x_i + e_i$ for some $e_1, ..., e_n \in \mathbb{R}^d$. 
Let $\nu = \max_i \|e_i\| < \infty$ be bounded. 
Define $\tilde{H}_n$ as the event in which an $(\eta + 2 \nu)$-neighborhood graph constructed from sample $y_1, ..., y_n$ is connected. 
Then for any $\epsilon > 0$, there exists $N = O \bigg( \frac{\log \epsilon + r \log \eta - \frac{r}{2} \log r}{\log (1 - a \eta^r r^{-r / 2})} \bigg)$ such that $P(\tilde{H}_n) > 1 - \epsilon$ when $n \geq N$.
\end{lemma}

\begin{proof}
\end{proof}

\begin{lemma}
\label{lem:K-hypercubes}
Let there be $K \geq 2$ hypercubes $\mathcal{C}_1, ..., \mathcal{C}_K$ of dimension $r_1, ..., r_K$ in $\mathbb{R}^d$ such that $\delta = \min\limits_{k \neq \ell} \min\limits_{x_i \in \mathcal{C}_k, x_j \in \mathcal{C}_\ell} \|x_i - x_j\| > 0$. 
Let $F_k$ be a distribution with support $C_k$ such that its density $f_k$ is nonzero on the support. Let $a = \min_k \min_t f_k(t) > 0$. 
Define a mixture model as follows: 

\begin{enumerate}
  \item Draw labels $z_1, ..., z_n \iid \Multinomial(\alpha_1, ..., \alpha_K)$. 
  \item Draw latent vectors each as $x_i \indep F_{z_i}$.
\end{enumerate}

Define $H'_n$ as the event that an $\eta$-neighborhood graph constructed from this sample consists of exactly $K$ disconnected subgraphs for any $\eta \in (0, \delta)$. 
Then for any $\epsilon \in (0, 1)$, there exists an $N = O \Big( \frac{\log(1 - (1 - \epsilon)^{1/K}) + d \log \eta - \frac{d}{2} \log d}{\alpha_{min} \log(1 - a \eta^d d^{-d/2})} \Big)$ such that when $n > N$, $P(E_n) > 1 - \epsilon$. 
\end{lemma}

\begin{proof}
Let $H^{(k)}$ be the event that lemma \ref{lem:one-hypercube} holds for $\mathcal{C}_k$. Then $H^{(k)} = H_{n_k}$ where $H_n$ is defined as in lemma \ref{lem:one-hypercube}. 
Then $P(H'_n) = P(H_{n_1} \text{ and } ... \text{ and } H_{n_K})$. 

$$
\begin{aligned}
P(H'_n) & = \prod_k P(H_{n_k}) \\
& \geq \prod_k 1 - \lceil \sqrt{r_k} / \eta \rceil^{r_k} (1 - a \eta^{r_k} r_k^{-r_k/2})^{n_k} \\
& \geq \prod_k 1 - \lceil \sqrt{d} / \eta \rceil^{d} (1 - a \eta^{d} d^{-d/2})^{\alpha_{\min} n} \\
& \geq \big(1 - \lceil \sqrt{d} / \eta \rceil^{d} (1 - a \eta^{d} d^{-d/2})^{\alpha_{\min} n} \big)^K, \\
\end{aligned}
$$
which approaches 1 as $n \to \infty$. 
Setting this quantity to $\geq 1 - \epsilon$ and solving for $n$ yields the desired rate. 
\end{proof}

\begin{proof}[Proof of theorem \ref{nonintersect-no-noise}]
\end{proof}

It can similarly be shown that the scenario in lemma \ref{lem:K-hypercubes} with bounded noise can be achieved by replacing the $\eta$-neighborhood graph with an $(\eta + 2 \nu)$-neighborhood graph, where $\nu = \max_i \|e_i\|$. 
This time, it is necessary to bound the noise as $\nu < \delta / 3$ to maintain separation between the hypercubes. 

\section{Details on Fitting Bezier Curves with Noise}